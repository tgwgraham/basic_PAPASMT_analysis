# This script combines only those movies for which the total number of localizations per 
# area of the ROI is below some threshold value
#
# It requires that the target folder contain an input dataframe containing DR localization counts
# (vcounts) and cell areas (area)
# This can be generated by running the notebook plot_GV_ratio.ipynb.
# 

import os, numpy as np, pandas as pd
from glob import glob
from saspt import StateArray, StateArrayDataset, RBME, load_detections

def getInputFiles(basefname,drpapa,countthresh):
    gv = pd.read_csv(basefname + '/locsperframe/gv_by_cell.csv')
    gv['vcounts_per_area'] = gv['vcounts']/gv['area']
    sel = gv['vcounts_per_area'] < countthresh
    input_files = []
    for index, row in gv[sel].iterrows():
        fovnum = row['fovnum']
        cellnum = row['cellnum']
        input_files.append(f'{basefname}/sortedTrajectories/cond1/exp1/{drpapa}/{fovnum}_{cellnum}.csv')
    return input_files

basefnames = [
'/mnt/g/SPT_analysis/20231030_LARP7_hnRNP_PAPA_and_controls/run2_1055/analysis20231101/',
'/mnt/g/SPT_analysis/20231113_LARP7_hnRNPA1_PAPA_and_ctrl/run1_hnRNPA1/analysis20231209/'
]

countthresh = 0.12

for drpapa in ['DR','PAPA']:

    input_files = []
    
    for basefname in basefnames:
        input_files.extend(getInputFiles(basefname,drpapa,countthresh))

    print('Number of movies = ', len(input_files))
        
    detections = load_detections(*input_files)
    settings = dict(
        likelihood_type = RBME,
        pixel_size_um = 0.16,
        frame_interval = 0.00748,
        focal_depth = 0.7,
        start_frame = 0,
        progress_bar = True,
        sample_size = 1e9,
    )
    SA = StateArray.from_detections(detections, **settings)
    with open(f'trajinfo_{drpapa}.txt','w') as fh:
        fh.write(f'Number of movies = {len(input_files)}\n')
        print(SA)
        print("Trajectory statistics:")
        fh.write(str(SA))
        fh.write('\n')
        fh.write('Trajectory statistics:\n')
        for k, v in SA.trajectories.processed_track_statistics.items():
            print(f"{k : <20} : {v}")
            fh.write(f"{k : <20} : {v}\n")
    prior_occs = SA.prior_occs
    naive_occs = SA.naive_occs
    posterior_occs = SA.posterior_occs
    marginal_posterior_occs = SA.marginalize_on_diff_coef(posterior_occs)
    
    SA.occupations_dataframe.to_csv(f"posterior_occupations_{drpapa}.csv", index=False)
    SA.plot_occupations(f"posterior_occupations_{drpapa}.png")
    SA.plot_assignment_probabilities(f"assignment_probabilities_{drpapa}.png")
    SA.plot_temporal_assignment_probabilities(f"assignment_probabilities_by_frame_{drpapa}.png")
    
